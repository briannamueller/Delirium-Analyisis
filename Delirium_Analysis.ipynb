{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules and merge the labs, demographics, and DX csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import RepeatedKFold, ShuffleSplit, KFold, RepeatedStratifiedKFold, cross_val_predict\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import scipy.stats\n",
    "\n",
    "df = pd.read_csv(\"one_encounter_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to drop Barthel index\n",
    "df.drop(\"Barthel\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate dataset as response variable and feature variables\n",
    "X = df.loc[:, ~df.columns.str.contains('DX')]\n",
    "y = df['DX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold):\n",
    "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])\n",
    "    \n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return (m-h, m+h)\n",
    "\n",
    "\n",
    "# Set up inner and outer CV loops for nested CV\n",
    "cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "cv_outer = RepeatedStratifiedKFold(n_splits=10, random_state=5)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "param_grid = {}\n",
    "logistic = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "# Random Forest\n",
    "model = RandomForestClassifier(max_features = 'sqrt', min_samples_split = 20, n_jobs = 32,\n",
    "                               min_samples_leaf = 2, criterion = 'entropy')\n",
    "param_grid = {'n_estimators': [200, 300], 'max_depth' : [10,11,12],'max_samples' : [.25,.5,.75]}\n",
    "rfc = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "# Gradient boosted model \n",
    "model = GradientBoostingClassifier(min_samples_split=20, min_samples_leaf=2, subsample=.5, max_features='sqrt',random_state=10)\n",
    "param_grid = {\"n_estimators\" : [300,400], \"learning_rate\" : [.01, 0.1],\"max_depth\":[7,8,9]}\n",
    "gbm = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "    \n",
    "# KNN\n",
    "model = KNeighborsClassifier(n_neighbors=200)\n",
    "param_grid = {'n_neighbors' : [200,300,400]}\n",
    "KNN = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "    \n",
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "param_grid = {}\n",
    "NB = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "# Adaboost\n",
    "model = Ada=AdaBoostClassifier()\n",
    "param_grid = {'n_estimators':[300,400],'learning_rate':[.05, .1]}\n",
    "Ada = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "# Decision Tree\n",
    "model = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "param_grid = {'max_depth' : [8,9,10], 'min_samples_leaf' : [5, 10], 'min_samples_split' : [20, 35, 50]}\n",
    "DT = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "from sklearn import svm\n",
    "model = svm.SVC(probability = True, kernel = 'rbf')\n",
    "param_grid = {'gamma' : [.1,1], 'C': [.1,1,10]}\n",
    "SVM = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv_inner, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up table for specified sensitivity\n",
    "sensitivity_result_table = pd.DataFrame(columns=['classifiers', 'accuracy', 'fpr','tpr','auc', 'sensitivity', 'specificity','ROC_plot'])\n",
    "# Set up table for specified specificity\n",
    "specificity_result_table = pd.DataFrame(columns=['classifiers','accuracy', 'fpr','tpr','auc', 'sensitivity', 'specificity','ROC_plot'])\n",
    "\n",
    "classifiers = [rfc, gbm,logistic, KNN, DT,NB,SVM]\n",
    "\n",
    "# Train the models and record the results\n",
    "for cls in classifiers:\n",
    "    \n",
    "    # Calculate AUC \n",
    "    auc = cross_val_score(cls, X, y, scoring='roc_auc', cv=cv_outer, n_jobs=-1)\n",
    "    auc_mean = np.mean(auc)\n",
    "    auc_CI = mean_confidence_interval(auc, confidence=0.95)\n",
    "    auc_CI = tuple([float(\"{0:.3f}\".format(n)) for n in auc_CI])\n",
    "    \n",
    "    # Get true positive rate and false positive rate for sensitivity / specificity thresholds\n",
    "    yproba = cross_val_predict(cls, X, y, cv=10, method='predict_proba')[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y,  yproba)\n",
    "    pred_labels = np.where(yproba > 0.5, 1, 0)\n",
    "    model_name = cls.estimator.__class__.__name__\n",
    "    print(f'Confusion Matrix and Classification Report for {model_name}:')\n",
    "    print(confusion_matrix(y, pred_labels))\n",
    "    print(classification_report(y, pred_labels))\n",
    "    print()\n",
    "    \n",
    "    # Get accuracy score using the predictions and true labels\n",
    "    accuracy = round(metrics.accuracy_score(y, pred_labels),2)\n",
    "\n",
    "    \n",
    "    # Define sensitivity and specificty thresholds to evaluate\n",
    "    select_sensitivity = .9\n",
    "    select_specificity = .5\n",
    "    x_fpr = fpr\n",
    "    y_tpr = tpr\n",
    "    \n",
    "    # Determine specificities at a set sensitivity value\n",
    "    specificity = 1 - np.interp(select_sensitivity, y_tpr, x_fpr) \n",
    "    # Determine sensitivities at a set sensitivity value\n",
    "    sensitivity = np.interp(1 - select_specificity, x_fpr, y_tpr)\n",
    "    \n",
    "    # Calculate CI intervals for sensitivity theshold adjustment \n",
    "    specificity_CI = proportion_confint(count= len(df)*specificity, nobs = len(df), alpha=.05)\n",
    "    specificity_CI = tuple([float(\"{0:.3f}\".format(n)) for n in specificity_CI])\n",
    "    sensitivity_CI = proportion_confint(count = len(df)*select_sensitivity, nobs = len(df), alpha=.05)\n",
    "    sensitivity_CI = tuple([float(\"{0:.3f}\".format(n)) for n in sensitivity_CI])\n",
    "    \n",
    "    sensitivity_result_table = sensitivity_result_table.append({'classifiers':cls.estimator.__class__.__name__,\n",
    "                                        'fpr':list(fpr), \n",
    "                                        'tpr':list(tpr), \n",
    "                                        'auc':f'{round(auc_mean,3)} {auc_CI}',\n",
    "                                        'ROC_plot': round(auc_mean,3),\n",
    "                                        'sensitivity': f'{select_sensitivity} {sensitivity_CI}',\n",
    "                                        'specificity' : f'{round(specificity,2)} {specificity_CI}',\n",
    "                                        'accuracy' : accuracy}, ignore_index=True)\n",
    "                \n",
    "\n",
    "    \n",
    "    # Calculate CI intervals for specificity threshold adjustment \n",
    "    specificity_CI = proportion_confint(count= len(df)*select_specificity, nobs = len(df), alpha=.05)\n",
    "    specificity_CI = tuple([float(\"{0:.3f}\".format(n)) for n in specificity_CI])\n",
    "    sensitivity_CI = proportion_confint(count = len(df)*sensitivity, nobs = len(df), alpha=.05)\n",
    "    sensitivity_CI = tuple([float(\"{0:.3f}\".format(n)) for n in sensitivity_CI])\n",
    "    \n",
    "    specificity_result_table = specificity_result_table.append({'classifiers':cls.estimator.__class__.__name__,\n",
    "                                        'fpr':list(fpr), \n",
    "                                        'tpr':list(tpr), \n",
    "                                        'auc':f'{round(auc_mean,3)} {auc_CI}',\n",
    "                                        'ROC_plot': round(auc_mean,3),\n",
    "                                        'sensitivity': f'{round(sensitivity,2)} {sensitivity_CI}',\n",
    "                                        'specificity' : f'{select_specificity} {specificity_CI}',\n",
    "                                        'accuracy' : accuracy}, ignore_index=True)\n",
    "\n",
    "specificity_result_table.set_index('classifiers', inplace=True)\n",
    "sensitivity_result_table.set_index('classifiers', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_result_table\n",
    "specificity_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "specificity_result_table.to_csv('specificity_one_encounter')\n",
    "specificity_result_table = pd.read_csv(\"specificity_one_encounter.csv\", index_col = 0)\n",
    "\n",
    "# Plot ROC curve and save image\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "for i in specificity_result_table.index[:7]:\n",
    "    a = ast.literal_eval(specificity_result_table.loc[i]['fpr'])\n",
    "    b = ast.literal_eval(specificity_result_table.loc[i]['tpr'])\n",
    "    plt.plot(a, b, linewidth=3.0,\n",
    "             label=\"{}, AUC={:.3f}\".format(i, specificity_result_table.loc[i]['ROC_plot']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1), fontsize = 15)\n",
    "plt.xlabel(\"Flase Positive Rate\", fontsize=20, labelpad=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1), fontsize = 15)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=20, labelpad=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontsize=20, pad = 15, fontweight='bold')\n",
    "plt.legend(prop={'size':15}, loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('ROC_one_encounter_no_barthel.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model Coefficients and Odds Ratios from Logistic Regression\n",
    "logistic = LogisticRegression(random_state = 1)\n",
    "logistic.fit(X, y)\n",
    "\n",
    "\n",
    "res = sm.Logit(y, sm.add_constant(X)).fit()\n",
    "params = res.params\n",
    "conf = res.conf_int()\n",
    "conf['Odds Ratio'] = params\n",
    "conf.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
    "conf = round(np.exp(conf),2)\n",
    "conf['95% CI'] = [f'{j[0]}-{j[1]}' for j in conf[['2.5%', '97.5%']].values] \n",
    "conf.to_csv('Odds_Ratio.csv')\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Final Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth' : [10,11,12],\n",
    "    'min_samples_leaf' : [2,10],\n",
    "    'min_samples_split' : [10,25],\n",
    "    'max_samples' : [.25,.5]\n",
    "}\n",
    "\n",
    "rfc = GridSearchCV(RandomForestClassifier(min_samples_split = 20, n_jobs = 32),\n",
    "                                                   param_grid, scoring = 'roc_auc', cv = 10)\n",
    "rfc.fit(X, y)\n",
    "rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with Barthel\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_features = 'sqrt', max_depth = 10,\n",
    "                            min_samples_split = 10, n_jobs = 32, min_samples_leaf = 2,\n",
    "                            criterion = 'entropy', max_samples = .25, random_state=10)\n",
    "\n",
    "rfc.fit(X, y)\n",
    "feature_importances = pd.DataFrame(rfc.feature_importances_, index = X.columns,\n",
    "                      columns=['importance']).sort_values('importance', ascending=False) \n",
    "\n",
    "Barthel = feature_importances.loc['Barthel', 'importance']\n",
    "feature_importances['Scaled Relative Importance'] = feature_importances['importance'].apply(lambda x : round((x/Barthel)*100,1))\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without Barthel\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_features = 'sqrt', max_depth = 10,\n",
    "                            min_samples_split = 10, n_jobs = 32, min_samples_leaf = 2,\n",
    "                            criterion = 'entropy', max_samples = .25, random_state=10)\n",
    "\n",
    "rfc.fit(X, y)\n",
    "feature_importances = pd.DataFrame(rfc.feature_importances_, index = X.columns,\n",
    "                      columns=['importance']).sort_values('importance', ascending=False) \n",
    "\n",
    "Dementia = feature_importances.loc['Dementia', 'importance']\n",
    "feature_importances['Scaled Relative Importance'] = feature_importances['importance'].apply(lambda x : round((x/Dementia)*100,1))\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on entire dataset to get optimal parameters for the final model\n",
    "# Train final model on all data\n",
    "# Get relative importances\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\" : [300,400],\n",
    "    \"learning_rate\" : [.01, 0.1],\n",
    "    \"max_depth\":[7,8,9],\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(GradientBoostingClassifier(n_estimators = 100), parameters,\n",
    "                   cv=10, scoring = 'roc_auc', n_jobs = 32)\n",
    "\n",
    "gbm.fit(X, y)\n",
    "gbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with Barthel\n",
    "gbm = GradientBoostingClassifier(n_estimators=400, learning_rate=.01,\n",
    "     max_depth=7, min_samples_split=20, min_samples_leaf=2, subsample=.25, max_features='sqrt',random_state=10)\n",
    "\n",
    "\n",
    "gbm.fit(X, y)\n",
    "feature_importances = pd.DataFrame(gbm.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False) \n",
    "\n",
    "Barthel = feature_importances.loc['Barthel', 'importance']\n",
    "feature_importances['Scaled Relative Importance'] = feature_importances['importance'].apply(lambda x : round((x/Barthel)*100,1))\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without Barthel\n",
    "gbm = GradientBoostingClassifier(n_estimators=400, learning_rate=.01,\n",
    "     max_depth=7, min_samples_split=20, min_samples_leaf=2, subsample=.25, max_features='sqrt',random_state=10)\n",
    "\n",
    "\n",
    "gbm.fit(X, y)\n",
    "feature_importances = pd.DataFrame(gbm.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False) \n",
    "\n",
    "Dementia = feature_importances.loc['Dementia', 'importance']\n",
    "feature_importances['Scaled Relative Importance'] = feature_importances['importance'].apply(lambda x : round((x/Dementia)*100,1))\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors' : [200,300,400]}\n",
    "KNN = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=10, refit=True)\n",
    "KNN.fit(X,y)\n",
    "KNN.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=300)\n",
    "KNN.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = GaussianNB()\n",
    "NB.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'max_depth' : [8,9,10],\n",
    "    'min_samples_leaf' : [2, 10], \n",
    "    'min_samples_split' : [20, 35, 50],\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "DT = GridSearchCV(sklearn.tree.DecisionTreeClassifier(), \n",
    "                  param_grid, scoring = 'roc_auc', cv = 10)\n",
    "DT.fit(X, y)\n",
    "DT.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 8,\n",
    "                                        min_samples_split = 50, min_samples_leaf = 2, random_state = 10)\n",
    "DT.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(probability = True, kernel = 'rbf')\n",
    "param_grid = {'gamma' : [.1,1], 'C': [.1,1,10]}\n",
    "\n",
    "SVM = GridSearchCV(model, \n",
    "                  param_grid, scoring = 'roc_auc', cv = 10)\n",
    "SVM.fit(X, y)\n",
    "SVM.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(gamma=.1, C=1, probability = True, kernel = 'rbf')\n",
    "svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictied Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = pd.read_csv('test_cases.csv', index_col = 0)\n",
    "test_cases = pd.read_csv('test_cases_no_barthel.csv', index_col = 0)\n",
    "\n",
    "pred_log = logistic.predict_proba(test_cases)[:,1]\n",
    "pred_rfc = rfc.predict_proba(test_cases)[:,1]\n",
    "pred_gbm = gbm.predict_proba(test_cases)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = pd.DataFrame(data = {'RF':pred_rfc,\n",
    "                                               'GBM':pred_gbm,\n",
    "                                               'LR':pred_log})\n",
    "predicted_probabilities.index = ['Case 1', 'Case 2', 'Case 3'] \n",
    "predicted_probabilities.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
